Data Catalogue and Processing K100 R53
======================================

Ok, time to get back into the data processing for the K100.  I came some distance in this when
working at Fermilab last summer, but now I have to close the loop and make the processing
automated.  Just to go over all the steps we need I'll list here:

1. copy raw data from `Vuk-01.spa.umn.edu` to `/data/chocula/`
2. change data over to IZIP format?  -- this really hurts us, can we get rid of it?
3. copy the raw data to offsite locations like `nero`/`nerva`/`galba.stanford.edu`
4. make appropriate directory structures on `/data/chocula/`
5. process the data in one of the locations
6. merge data from the above processing
7. sync with other locations


So, I've been fishing around for a while to find where all my code related to this is, here is a
list.

* on `vegemite.spa.umn.edu` at `umn_work/cdms_analysis/cpp/s34/` we have scripts for copying UMN
    data and making directory structures.  This is in my CVS code_repository. 
* on `cdmsmicro.fnal.gov` at `/localhome/cdms/user_dir/villaa/R46_proc_cvs` I have the processing
    scripts and the versions of cdmsbats I've used for the most recent processing of R46 data.
    Oddly it says that this should be in my code repository at `umn_work/R46_proc`, but I can't
    find it there -- actually update, it's there but not checked out on `vegemite`.
* on `vegemite.spa.umn.edu` at `umn_work/cdms_analysis/python/s34/newProcessing` is my new scripts
    for processing similar to Jianjie's method but a step toward transparency.  Using condor. 

Organization
------------

I think this thing can be written largely in python, that's probably best.  But I should think
carefully about how to break the python scripts up.  Generically, I think one script/library can
be called the copy/organization library and one the "shell" processing, and one the "condor"
processing.  The shell processing library should be bare-bones and hide as little from the user as
possible.  The condor library can hide more, but needs to keep logs.  Libraries for other
pipleines can be added pretty easily by adding a new .py file.  The .py files can also be used as
libraries so copying/processing/analysis/DQ tasks can be tailored to specific needs. 

Instructions for Copy/Process Scripts
=====================================

I'll list here a pseudo manual for using the copy/processing scripts I am developing, so that I
can turn it into a real manual when I'm done. 

1. First I made it so that I could ssh to `Vuk-01.spa.umn.edu` without a password by an rsa key
pair.  I did this by taking my file `~/.ssh/new_umn_rsa.pub` and placing into the
`~/.ssh/authorized_keys` file on the machine `Vuk-01.spa.umn.edu` in the `Administrator` home
directory.  Always log into this machine as `Administrator`
